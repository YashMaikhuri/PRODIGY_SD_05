{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcca8210-d6a8-4145-b821-5e5acdcd3fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\yashm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\yashm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yashm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yashm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yashm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yashm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\yashm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (2.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping http://books.toscrape.com/catalogue/page-1.html...\n",
      "Scraping http://books.toscrape.com/catalogue/page-2.html...\n",
      "Scraping http://books.toscrape.com/catalogue/page-3.html...\n",
      "Scraping http://books.toscrape.com/catalogue/page-4.html...\n",
      "Scraping http://books.toscrape.com/catalogue/page-5.html...\n",
      "Data saved to books_data.csv with 100 products.\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = \"http://books.toscrape.com/catalogue/page-1.html\"\n",
    "\n",
    "# Function to get the HTML content of the page\n",
    "def get_page_content(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.content\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to parse the page and extract product information\n",
    "def parse_page(content):\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    products = []\n",
    "    \n",
    "    # Find all product elements\n",
    "    books = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "    \n",
    "    for book in books:\n",
    "        # Extract the book title\n",
    "        title = book.h3.a[\"title\"]\n",
    "        \n",
    "        # Extract the price\n",
    "        price = book.find(\"p\", class_=\"price_color\").text.strip()\n",
    "        \n",
    "        # Extract the rating (as a class, e.g., \"star-rating Four\")\n",
    "        rating = book.p[\"class\"][1]  # e.g., \"One\", \"Two\", \"Three\", etc.\n",
    "        \n",
    "        products.append([title, price, rating])\n",
    "    \n",
    "    return products\n",
    "\n",
    "# Function to save product data to a CSV file\n",
    "def save_to_csv(products, filename):\n",
    "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Title\", \"Price\", \"Rating\"])  # Header row\n",
    "        writer.writerows(products)\n",
    "\n",
    "# Main function to scrape multiple pages and store the data\n",
    "def scrape_website(base_url, pages=1):\n",
    "    all_products = []\n",
    "    \n",
    "    # Scrape multiple pages if required\n",
    "    for page_num in range(1, pages + 1):\n",
    "        page_url = f\"{base_url.replace('page-1', f'page-{page_num}')}\"\n",
    "        print(f\"Scraping {page_url}...\")\n",
    "        content = get_page_content(page_url)\n",
    "        \n",
    "        if content:\n",
    "            products = parse_page(content)\n",
    "            all_products.extend(products)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve content from {page_url}\")\n",
    "    \n",
    "    # Save the data to CSV\n",
    "    save_to_csv(all_products, \"books_data.csv\")\n",
    "    print(f\"Data saved to books_data.csv with {len(all_products)} products.\")\n",
    "\n",
    "# Start the scraping process\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = \"http://books.toscrape.com/catalogue/page-1.html\"\n",
    "    scrape_website(base_url, pages=5)  # Scrape the first 5 pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3610b0e-1129-4108-9a27-41ee6aa4990c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
